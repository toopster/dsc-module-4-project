{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science - Module 4 - Final Project Submission\n",
    "\n",
    "* Student Name: **James Toop**\n",
    "* Student Pace: **Self Paced**\n",
    "* Scheduled project review date/time: **Wednesday, 8th September 2021 - 9.30pm BST**\n",
    "* Instructor name: **Jeff Herman**\n",
    "* Blog post URL: **https://toopster.github.io**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Business Case](#business-case)\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "    1. [Discovery](#data-discovery)\n",
    "    2. [Preprocessing](#data-preprocessing)\n",
    "3. [Deep Learning Neural Networks](#deep-learning-neural-networks)\n",
    "    1. [Model 1: Create a baseline network](#model-1)\n",
    "    2. [Model 2: Deepen the network and increase the number of neurons in each layer](#model-2)\n",
    "    3. [Model 3: A deeper network but with a different activation type and reduce the number of neurons](#model-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"business-case\"></a>\n",
    "## 1. Business Case and Project Purpose\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"eda\"></a>\n",
    "## 2. Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"data-discovery\"></a>\n",
    "### 2A. Data Discovery\n",
    "\n",
    "This section presents an initial step to investigate, understand and document the available data fields and relationships, highlighting any potential issues / shortcomings within the datasets supplied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory structure for images\n",
    "train_folder = 'chest_xray/train/'\n",
    "train_normal = 'chest_xray/train/NORMAL/'\n",
    "train_pneumonia = 'chest_xray/train/PNEUMONIA/'\n",
    "\n",
    "# Store all the relevant image names in specific objects\n",
    "train_images_normal = [file for file in os.listdir(train_normal) if file.endswith('.jpeg')]\n",
    "train_images_pneumonia = [file for file in os.listdir(train_pneumonia) if file.endswith('.jpeg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NORMAL2-IM-0927-0001.jpeg',\n",
       " 'NORMAL2-IM-1056-0001.jpeg',\n",
       " 'IM-0427-0001.jpeg',\n",
       " 'NORMAL2-IM-1260-0001.jpeg',\n",
       " 'IM-0656-0001-0001.jpeg',\n",
       " 'IM-0561-0001.jpeg',\n",
       " 'NORMAL2-IM-1110-0001.jpeg',\n",
       " 'IM-0757-0001.jpeg',\n",
       " 'NORMAL2-IM-1326-0001.jpeg',\n",
       " 'NORMAL2-IM-0736-0001.jpeg']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview filenames for \"normal\" training images\n",
    "train_images_normal[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person63_bacteria_306.jpeg',\n",
       " 'person1438_bacteria_3721.jpeg',\n",
       " 'person755_bacteria_2659.jpeg',\n",
       " 'person478_virus_975.jpeg',\n",
       " 'person661_bacteria_2553.jpeg',\n",
       " 'person276_bacteria_1296.jpeg',\n",
       " 'person1214_bacteria_3166.jpeg',\n",
       " 'person1353_virus_2333.jpeg',\n",
       " 'person26_bacteria_122.jpeg',\n",
       " 'person124_virus_238.jpeg']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview filenames for \"pneumonia\" training images\n",
    "train_images_pneumonia[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ascertain the size of the training dataset\n",
    "print('Number of training chest x-ray images that are normal:', len(train_images_normal))\n",
    "print('Number of training chest x-ray images that have pneumonia:', len(train_images_pneumonia))\n",
    "print('\\nTotal training chest x-ray images:', len(train_images_normal)+len(train_images_pneumonia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory structure for images\n",
    "test_folder = 'chest_xray/test/'\n",
    "test_normal = 'chest_xray/test/NORMAL/'\n",
    "test_pneumonia = 'chest_xray/test/PNEUMONIA/'\n",
    "\n",
    "# Store all the relevant image names in specific objects\n",
    "test_images_normal = [file for file in os.listdir(test_normal) if file.endswith('.jpeg')]\n",
    "test_images_pneumonia = [file for file in os.listdir(test_pneumonia) if file.endswith('.jpeg')]\n",
    "\n",
    "# Ascertain the size of the test dataset\n",
    "print('Number of test chest x-ray images that are normal:', len(test_images_normal))\n",
    "print('Number of test chest x-ray images that have pneumonia:', len(test_images_pneumonia))\n",
    "print('\\nTotal test chest x-ray images:', len(test_images_normal)+len(test_images_pneumonia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory structure for images\n",
    "val_folder = 'chest_xray/val/'\n",
    "val_normal = 'chest_xray/val/NORMAL/'\n",
    "val_pneumonia = 'chest_xray/val/PNEUMONIA/'\n",
    "\n",
    "# Store all the relevant image names in specific objects\n",
    "val_images_normal = [file for file in os.listdir(val_normal) if file.endswith('.jpeg')]\n",
    "val_images_pneumonia = [file for file in os.listdir(val_pneumonia) if file.endswith('.jpeg')]\n",
    "\n",
    "# Ascertain the size of the validation dataset\n",
    "print('Number of validation chest x-ray images that are normal:', len(val_images_normal))\n",
    "print('Number of validation chest x-ray images that have pneumonia:', len(val_images_pneumonia))\n",
    "print('\\nTotal validation chest x-ray images:', len(val_images_normal)+len(val_images_pneumonia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"data-preprocessing\"></a>\n",
    "### 2B. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the data in the directory chest_xrays/train (5216 images), and reshape them\n",
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_folder, \n",
    "        target_size = (128, 128), \n",
    "        batch_size = 5216)\n",
    "\n",
    "# Get all the data in the directory chest_xrays/test (624 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size = (128, 128), \n",
    "        batch_size = 624) \n",
    "\n",
    "# Get all the data in the directory chest_xrays/validation (16 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_folder, \n",
    "        target_size = (128, 128), \n",
    "        batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)\n",
    "val_images, val_labels = next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset again\n",
    "m_train = train_images.shape[0]\n",
    "num_px = train_images.shape[1]\n",
    "m_test = test_images.shape[0]\n",
    "m_val = val_images.shape[0]\n",
    "\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"Number of testing samples: \" + str(m_test))\n",
    "print (\"Number of validation samples: \" + str(m_val))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "print (\"test_images shape: \" + str(test_images.shape))\n",
    "print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "print (\"val_images shape: \" + str(val_images.shape))\n",
    "print (\"val_labels shape: \" + str(val_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the training labels\n",
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "val_img = val_images.reshape(val_images.shape[0], -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(test_img.shape)\n",
    "print(val_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = train_img.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.reshape(train_labels[:,0], (5216,1))\n",
    "test_y = np.reshape(test_labels[:,0], (624,1))\n",
    "val_y = np.reshape(val_labels[:,0], (16,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for visualising results\n",
    "def visualize_results(results):\n",
    "    history = results.history\n",
    "\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.legend(['Validation Loss', 'Training Loss'], fontsize=12)\n",
    "    plt.title('Loss', fontsize=18)\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.plot(history['acc'])\n",
    "    plt.legend(['Validation Accuracy', 'Training Accuracy'], fontsize=12)\n",
    "    plt.title('Accuracy', fontsize=18)\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"deep-learning-neural-networks\"></a>\n",
    "# 3. Deep Learning Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model-1\"></a>\n",
    "### 3A. Model 1: Create a baseline network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Build a baseline model\n",
    "model_1 = models.Sequential()\n",
    "model_1.add(layers.Dense(64, activation='tanh', input_shape=(n_features,)))\n",
    "model_1.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# View summary for model\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the baseline model\n",
    "model_1.compile(loss='categorical_crossentropy', \n",
    "                optimizer='sgd', \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the baseline model\n",
    "results_1 = model_1.fit(train_img, \n",
    "                        train_labels, \n",
    "                        epochs=30, \n",
    "                        batch_size=64, \n",
    "                        validation_data=(test_img, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the loss and accuracy of the training and validation sets across epochs\n",
    "visualize_results(results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the training results\n",
    "results_1_train = model_1.evaluate(train_img, train_labels)\n",
    "results_1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the test results\n",
    "results_1_test = model_1.evaluate(test_img, test_labels)\n",
    "results_1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model-2\"></a>\n",
    "### 3B. Model 2: Deepen the network and increase the number of neurons in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Build a deeper model\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Dense(300, activation='tanh', input_shape=(n_features,)))\n",
    "model_2.add(layers.Dense(100, activation='tanh'))\n",
    "model_2.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# View summary for model\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the deeper model\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "                optimizer='sgd',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit the deeper model\n",
    "results_2 = model_2.fit(train_img, \n",
    "                        train_labels,\n",
    "                        batch_size=64, \n",
    "                        epochs=30, \n",
    "                        validation_data=(test_img, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the loss and accuracy of the training and validation sets across epochs\n",
    "visualize_results(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the training results\n",
    "results_2_train = model_2.evaluate(train_img, train_labels)\n",
    "results_2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the test results\n",
    "results_2_test = model_2.evaluate(test_img, test_labels)\n",
    "results_2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model-3\"></a>\n",
    "### 3C. Model 3: A deeper network but with a different activation type and reduce the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Build a deeper model with less neurons and change activation type\n",
    "model_3 = models.Sequential()\n",
    "model_3.add(layers.Dense(64, activation='relu', input_shape=(n_features,)))\n",
    "model_3.add(layers.Dense(32, activation='relu'))\n",
    "model_3.add(layers.Dense(16, activation='relu'))\n",
    "model_3.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_3.compile(loss='categorical_crossentropy',\n",
    "                optimizer='sgd',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "results_3 = model_3.fit(train_img, \n",
    "                        train_labels,\n",
    "                        batch_size=64, \n",
    "                        epochs=30, \n",
    "                        validation_data=(test_img, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the loss and accuracy of the training and validation sets across epochs\n",
    "visualize_results(results_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the training results\n",
    "results_3_train = model_3.evaluate(train_img, train_labels)\n",
    "results_3_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the test results\n",
    "results_3_test = model_3.evaluate(test_img, test_labels)\n",
    "results_3_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model-4\"></a>\n",
    "### 3D. Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regularizers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-3e56349d6648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Build a baseline model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel_4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'regularizers' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Build a baseline model\n",
    "model_4 = models.Sequential()\n",
    "model_4.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.005), input_shape=(n_features,)))\n",
    "model_4.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# View summary for model\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = keras.optimizers.SGD(0.001)\n",
    "\n",
    "model_4.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "results_4 = model_4.fit(train_img, \n",
    "                        train_labels,\n",
    "                        batch_size=64, \n",
    "                        epochs=30, \n",
    "                        validation_data=(test_img, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the loss and accuracy of the training and validation sets across epochs\n",
    "visualize_results(results_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the training results\n",
    "results_4_train = model_4.evaluate(train_img, train_labels)\n",
    "results_4_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the training results\n",
    "results_3_test = model_3.evaluate(test_img, test_labels)\n",
    "results_3_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model-X\"></a>\n",
    "### 3X. Model X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a baseline fully connected model\n",
    "\n",
    "model_X = models.Sequential()\n",
    "model_X.add(layers.Dense(20, activation='relu', input_shape=(n_features,))) # 2 hidden layers\n",
    "model_X.add(layers.Dense(7, activation='relu'))\n",
    "model_X.add(layers.Dense(5, activation='relu'))\n",
    "model_X.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_X.compile(loss='binary_crossentropy',\n",
    "                optimizer='sgd',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_X = model_X.fit(train_img,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_img, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(results_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_X_train = model_X.evaluate(train_img, train_y)\n",
    "results_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_X_test = model_X.evaluate(test_img, test_y)\n",
    "results_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model-X\"></a>\n",
    "### 3Y. CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Y = models.Sequential()\n",
    "model_Y.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))\n",
    "model_Y.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model_Y.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model_Y.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model_Y.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_Y.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model_Y.add(layers.Flatten())\n",
    "model_Y.add(layers.Dense(64, activation='relu'))\n",
    "model_Y.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_Y.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "results_Y = model.fit(train_img,\n",
    "                    train_labels,\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_images, val_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
